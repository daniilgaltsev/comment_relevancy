{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd83fe1-766b-49b2-bc57-0132d31fb61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada80f8-3cdc-43d6-b39e-5e8a9b217ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"role_domain_1000_10000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c901282-cea6-4313-9799-d612131f263b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9000"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with Path(filename).open(\"rb\") as f:\n",
    "    results = pickle.load(f)\n",
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae604d8f-21b7-44f6-96e5-78a04086e77f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'succeeded',\n",
       " 'content': '{\\n  \"function_definition\": \"def transform_data(data: pd.DataFrame, missing_values_threshold: float = 0.5) -> pd.DataFrame:\",\\n  \"code\": \"\"\"\\n    # Check for missing values\\n    missing_values_count = data.isnull().sum()\\n    total_values_count = data.shape[0]\\n    missing_values_ratio = missing_values_count / total_values_count\\n\\n    # Drop columns with too many missing values\\n    drop_columns = missing_values_ratio[missing_values_ratio > missing_values_threshold].index\\n    data = data.drop(drop_columns, axis=1)\\n\\n    # Impute remaining missing values with the median\\n    data = data.fillna(data.median())\\n\\n    # Normalize the data\\n    scaler = StandardScaler()\\n    data[data.columns] = scaler.fit_transform(data)\\n\\n    return data\\n\"\"\",\\n  \"comment\": \"# Impute remaining missing values with the median\",\\n  \"explanation\": \"The comment correctly describes the imputation step, but it doesn\\'t mention that the data is also normalized after imputing the missing values.\",\\n  \"correct\": false\\n}'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(results.values()))\n",
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc10387e-0125-491a-a107-3802a1ac813a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8106\n",
      "function_definition\n",
      "\n",
      "def forecast_sales(sales_data: pd.DataFrame, forecast_horizon: int) -> pd.DataFrame:\n",
      "    \\\"\\\"\\\"\n",
      "    Forecast future sales using an ARIMA model.\n",
      "    \n",
      "    Args:\n",
      "        sales_data (pd.DataFrame): DataFrame containing historical sales data.\n",
      "        forecast_horizon (int): Number of periods to forecast.\n",
      "    \n",
      "    Returns:\n",
      "        pd.DataFrame: DataFrame containing forecasted sales.\n",
      "    \\\"\\\"\\\"\n",
      "\n",
      "code\n",
      "\n",
      "    sales_data = sales_data.set_index('date')\n",
      "    \n",
      "    # Fit an ARIMA model to the sales data\n",
      "    model = ARIMA(sales_data['sales'], order=(1, 1, 1))\n",
      "    model_fit = model.fit()\n",
      "    \n",
      "    # Generate forecasts for the desired horizon\n",
      "    forecast = model_fit.forecast(steps=forecast_horizon)\n",
      "    \n",
      "    # Create a DataFrame with the forecasted sales\n",
      "    forecast_df = pd.DataFrame({'date': pd.date_range(start=sales_data.index[-1] + pd.Timedelta(days=1), \n",
      "                                                     periods=forecast_horizon, \n",
      "                                                     freq='D'),\n",
      "                               'sales': forecast[0]})\n",
      "    forecast_df = forecast_df.set_index('date')\n",
      "    \n",
      "    return forecast_df\n",
      "\n",
      "comment\n",
      "\n",
      "# Fit an ARIMA model to the sales data\n",
      "\n",
      "explanation\n",
      "\"The comment accurately describes the purpose of the code block, which is to fit an ARIMA model to the historical sales data.\"\n",
      "\n",
      "correct\n",
      "True\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fd_key = '\"function_definition\": '\n",
    "code_key = '\"code\": '\n",
    "com_key = '\"comment\": '\n",
    "e_key = '\"explanation\": '\n",
    "r_key = '\"correct\": '\n",
    "\n",
    "check_keys = list(map(lambda x: x[1:-3], [fd_key, code_key, com_key, e_key, r_key]))\n",
    "\n",
    "def prepare_data(example):\n",
    "    if example[\"type\"] != \"succeeded\":\n",
    "        return None\n",
    "    cont = example[\"content\"]\n",
    "    \n",
    "    fd_s = cont.find(fd_key)\n",
    "    code_s = cont.find(code_key)\n",
    "    com_s = cont.find(com_key)\n",
    "    e_s = cont.find(e_key)\n",
    "    r_s = cont.find(r_key)\n",
    "\n",
    "    fd_e = cont.find('\",\\n', fd_s) + 1\n",
    "    code_e = cont.find('\",\\n', code_s) + 1\n",
    "    com_e = cont.find('\",\\n', com_s) + 1\n",
    "    e_e = cont.find('\",\\n', e_s) + 1\n",
    "    r_e = cont.find('\\n}', r_s)\n",
    "\n",
    "    if not (fd_s < fd_e < code_s < code_e < com_s < com_e < e_s < e_e < r_s < r_e):\n",
    "        return None\n",
    "\n",
    "    splits = [cont[fd_s:fd_e], cont[code_s:code_e], cont[com_s:com_e], cont[e_s:e_e], cont[r_s:r_e]]\n",
    "    prepared = {}\n",
    "    for k_v in splits:\n",
    "        split_p = k_v.find(\":\")\n",
    "        k, v = k_v[:split_p][1:-1], k_v[split_p+1:].strip()\n",
    "        # if not k in check_keys:\n",
    "        #     return False\n",
    "        assert k in check_keys\n",
    "\n",
    "        if k == \"correct\":\n",
    "            v = (v == \"true\")\n",
    "        else:\n",
    "            if v[:3] == '\"\"\"':\n",
    "                v = v[3:]\n",
    "            if v[-3:] == '\"\"\"':\n",
    "                v = v[:-3]\n",
    "            v = v.rstrip()\n",
    "        prepared[k] = v\n",
    "\n",
    "    return prepared\n",
    "\n",
    "dataset = []\n",
    "for v in results.values():\n",
    "    prepared = prepare_data(v)\n",
    "    if prepared:\n",
    "        dataset.append(prepared)\n",
    "        \n",
    "print(len(dataset))\n",
    "for k, v in dataset[-1].items():\n",
    "    print(k)\n",
    "    print(v)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadb587e-c468-48a2-9d85-2a6c69549989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4135 3971\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for example in dataset:\n",
    "    count += example[\"correct\"]\n",
    "print(count, len(dataset) - count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb3cc5-d251-4fa9-b806-0a8b2fa512ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Path(\"prepared_role_domain_1000_10000\").open(\"wb\") as f:\n",
    "    pickle.dump(dataset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e86cd-d8d6-41e0-a632-073d3e4ecac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'function_definition': '\"def transform_data(data: pd.DataFrame, missing_values_threshold: float = 0.5) -> pd.DataFrame:\"', 'code': '\\n    # Check for missing values\\n    missing_values_count = data.isnull().sum()\\n    total_values_count = data.shape[0]\\n    missing_values_ratio = missing_values_count / total_values_count\\n\\n    # Drop columns with too many missing values\\n    drop_columns = missing_values_ratio[missing_values_ratio > missing_values_threshold].index\\n    data = data.drop(drop_columns, axis=1)\\n\\n    # Impute remaining missing values with the median\\n    data = data.fillna(data.median())\\n\\n    # Normalize the data\\n    scaler = StandardScaler()\\n    data[data.columns] = scaler.fit_transform(data)\\n\\n    return data', 'comment': '\"# Impute remaining missing values with the median\"', 'explanation': '\"The comment correctly describes the imputation step, but it doesn\\'t mention that the data is also normalized after imputing the missing values.\"', 'correct': False}\n"
     ]
    }
   ],
   "source": [
    "with Path(\"prepared_role_domain_1000_10000\").open(\"rb\") as f:\n",
    "    print(pickle.load(f)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c9c0b0-a16f-4e5c-b3a1-1dabaa0ab954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
