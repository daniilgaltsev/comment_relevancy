{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ac1ef2-ad80-484f-b289-74c1c6a5aa34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194feb1-0c16-458a-854f-01bb3062abbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ce6c31-8a65-429e-893b-4aa50b3c5c6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token:  Paris\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_token_id = outputs.logits[0, masked_index].argmax(axis=-1)\n",
    "predicted_token = tokenizer.decode(predicted_token_id)\n",
    "print(\"Predicted token:\", predicted_token)\n",
    "# Predicted token:  Paris\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba8a55-220e-42d6-8c56-cd524ddd73f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f7720c-bd3b-4fc8-8e0e-b230605cea31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS]Write the the sentence sentence\\n\\n to English.[SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_tokens_id = outputs.logits[0].argmax(axis=-1)\n",
    "predicted_tokens = tokenizer.decode(predicted_tokens_id)\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcba1961-df8b-4a53-a7e9-e9c832a2e0a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtoken_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'np.ndarray'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'torch.Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mForwardRef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tf.Tensor'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Converts a sequence of ids in a string, using the tokenizer and vocabulary with options to remove special\n",
       "tokens and clean up tokenization spaces.\n",
       "\n",
       "Similar to doing `self.convert_tokens_to_string(self.convert_ids_to_tokens(token_ids))`.\n",
       "\n",
       "Args:\n",
       "    token_ids (`Union[int, List[int], np.ndarray, torch.Tensor, tf.Tensor]`):\n",
       "        List of tokenized input ids. Can be obtained using the `__call__` method.\n",
       "    skip_special_tokens (`bool`, *optional*, defaults to `False`):\n",
       "        Whether or not to remove special tokens in the decoding.\n",
       "    clean_up_tokenization_spaces (`bool`, *optional*):\n",
       "        Whether or not to clean up the tokenization spaces. If `None`, will default to\n",
       "        `self.clean_up_tokenization_spaces`.\n",
       "    kwargs (additional keyword arguments, *optional*):\n",
       "        Will be passed to the underlying model specific decode method.\n",
       "\n",
       "Returns:\n",
       "    `str`: The decoded sentence.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniforge3/envs/comrel/lib/python3.11/site-packages/transformers/tokenization_utils_base.py\n",
       "\u001b[0;31mType:\u001b[0m      method"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer.decode?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bbae28-50b2-4cca-b3bc-50cd2e46e4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0478454f07b8484da725148b175a8cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c5d02fb3ed64ba19be8b7be62f6f500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837b3a82546641afa2e7126c6d5a71a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7de12151f0c7430f9548c0337fed43af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfd70e83a5234a2b867d202ffd4ece52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45a6ff4c996f40c39b4fadfe09bc02fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75a3a41b363a4168969a41fbdd8d35a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1bb43ab505a41299bbc7c39ce05b8ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between the mass of an object and the distance between it and the center of the Earth. The force of gravity\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "\n",
    "device = \"cuda\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0793fc1a-f6bf-47aa-85a8-f579eb37983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0176251b-3f57-4baf-bbbd-1ca0de09ddbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5267718400000376\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. In the context of our universe, gravity is a result of the\n"
     ]
    }
   ],
   "source": [
    "st = perf_counter()\n",
    "outputs = model.generate(inputs, max_new_tokens=50, do_sample=False)\n",
    "print(perf_counter() - st)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69390502-419e-4189-b2e8-b35b391bcb04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.067973028999859\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. In the context of our universe, gravity is a result of the curvature of spacetime caused by the presence of mass and energy.\n",
      "\n",
      "Imagine spacetime as a trampoline. When you place a heavy object, like a bowling ball, on the trampoline, it creates a depression in the surface. This depression\n"
     ]
    }
   ],
   "source": [
    "st = perf_counter()\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "print(perf_counter() - st)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08350aa0-ba2c-4762-9d86-caf472c81356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.080845785000292\n",
      "<|im_start|>system\n",
      "You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>\n",
      "<|im_start|>user\n",
      "What is gravity?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Gravity is a fundamental force of nature that attracts objects with mass towards each other. It is a result of the interaction between mass, energy, and space itself. In the context of our universe, gravity is a result of the curvature of spacetime caused by the presence of mass and energy.\n",
      "\n",
      "Imagine spacetime as a trampoline. When you place a heavy object, like a bowling ball, on the trampoline, it creates a depression in the surface. This depression is caused by the object's mass and the energy it contains. The more massive the object, the larger the depression.\n",
      "\n",
      "Now, when you move an object, such as a bowling ball, it creates a gravitational pull on the surrounding space. This gravitational pull is what causes the bowling ball to move towards the center of the trampoline. The more massive the object, the stronger the gravitational pull.\n",
      "\n",
      "Gravity is a universal force that acts between all objects in the universe, regardless\n"
     ]
    }
   ],
   "source": [
    "st = perf_counter()\n",
    "outputs = model.generate(inputs, max_new_tokens=200, do_sample=False)\n",
    "print(perf_counter() - st)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3602cab4-6ec9-4576-9304-5513be2afc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 0.07136732900016796\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]Write the the sentence sentence\\n\\n to English.[SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"[MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK].\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "st = perf_counter()\n",
    "outputs = model(**inputs)\n",
    "print(inputs[\"input_ids\"].shape[1], perf_counter() - st)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_tokens_id = outputs.logits[0].argmax(axis=-1)\n",
    "predicted_tokens = tokenizer.decode(predicted_tokens_id)\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f8ad81-39d3-4c16-9491-b441e6dc2a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 0.19231256000011854\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]\\n\\n\\n\\n\\n[CLS][CLS]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n[SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP][SEP]\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n....\\n\\n\\n\\n\\n..........................\\n\\n....\\n\\n..[SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"[MASK]\" * 100\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "st = perf_counter()\n",
    "outputs = model(**inputs)\n",
    "print(inputs[\"input_ids\"].shape[1], perf_counter() - st)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_tokens_id = outputs.logits[0].argmax(axis=-1)\n",
    "predicted_tokens = tokenizer.decode(predicted_tokens_id)\n",
    "predicted_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d5d139-928d-40b3-afb2-294f3be3d4a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102 0.17626123499985624\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS]Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi!Hi, Hi![SEP]'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Hi, [MASK]!\" * 25\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "st = perf_counter()\n",
    "outputs = model(**inputs)\n",
    "print(inputs[\"input_ids\"].shape[1], perf_counter() - st)\n",
    "\n",
    "# To get predictions for the mask:\n",
    "masked_index = inputs[\"input_ids\"][0].tolist().index(tokenizer.mask_token_id)\n",
    "predicted_tokens_id = outputs.logits[0].argmax(axis=-1)\n",
    "predicted_tokens = tokenizer.decode(predicted_tokens_id)\n",
    "predicted_tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
